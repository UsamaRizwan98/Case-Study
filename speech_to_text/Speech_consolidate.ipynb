{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1e91edf",
      "metadata": {
        "id": "d1e91edf"
      },
      "source": [
        "# Audio Transcription with a Simple ASR Model\n",
        "This notebook trains and evaluates a basic **Audio-to-Text** (ASR) model using a small subset of the **LibriSpeech** dataset from HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc49e4f3",
      "metadata": {
        "id": "cc49e4f3"
      },
      "source": [
        "## 1) Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a270d2a",
      "metadata": {
        "id": "4a270d2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torch torchaudio transformers datasets matplotlib --quiet\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9630410",
      "metadata": {
        "id": "c9630410"
      },
      "source": [
        "## 2) Dataset Download (LibriSpeech Dummy via HuggingFace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef428fac",
      "metadata": {
        "id": "ef428fac"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Download a small ASR dataset from HuggingFace and save locally\n",
        "so train_asr.py can use it without re-downloading.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import torchaudio\n",
        "\n",
        "OUT_DIR = \"./data/asr\"\n",
        "DATASET_NAME = \"hf-internal-testing/librispeech_asr_dummy\"  # change to \"librispeech_asr\" for full set\n",
        "SPLITS = [\"train\", \"validation\"]\n",
        "\n",
        "def save_wav(audio, sample_rate, path):\n",
        "    torchaudio.save(path, audio, sample_rate)\n",
        "\n",
        "def main():\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    for split in SPLITS:\n",
        "        split_dir = os.path.join(OUT_DIR, split)\n",
        "        os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"[INFO] Downloading {DATASET_NAME} split: {split}\")\n",
        "        dataset = load_dataset(DATASET_NAME, split=split)\n",
        "\n",
        "        for i, item in enumerate(dataset):\n",
        "            audio = item[\"audio\"][\"array\"]\n",
        "            sr = item[\"audio\"][\"sampling_rate\"]\n",
        "            text = item[\"text\"]\n",
        "\n",
        "            wav_path = os.path.join(split_dir, f\"{i}.wav\")\n",
        "            txt_path = os.path.join(split_dir, f\"{i}.txt\")\n",
        "\n",
        "            save_wav(torchaudio.tensor(audio).unsqueeze(0), sr, wav_path)\n",
        "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(text)\n",
        "\n",
        "        print(f\"[INFO] Saved {len(dataset)} samples to {split_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec857197",
      "metadata": {
        "id": "ec857197"
      },
      "source": [
        "## 3) Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1a25d5",
      "metadata": {
        "id": "6e1a25d5"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch.utils.data import DataLoader\n",
        "# import torchaudio\n",
        "# from torchaudio.datasets import SPEECHCOMMANDS\n",
        "# from torchaudio.transforms import MelSpectrogram\n",
        "# import os\n",
        "\n",
        "# class FilteredSpeechCommands(SPEECHCOMMANDS):\n",
        "#     def __init__(self, root, subset=None, allowed_labels=None, n_mels=64):\n",
        "#         self.allowed_labels = allowed_labels\n",
        "#         self.mel_spec = MelSpectrogram(sample_rate=16000, n_mels=n_mels)\n",
        "\n",
        "#         dataset_path = os.path.join(root, \"speechcommands\")\n",
        "#         download_flag = not os.path.exists(dataset_path)\n",
        "#         super().__init__(root=dataset_path, download=download_flag, subset=subset)\n",
        "\n",
        "#         if self.allowed_labels:\n",
        "#             self._walker = [\n",
        "#                 w for w in self._walker\n",
        "#                 if os.path.basename(os.path.dirname(w)) in self.allowed_labels\n",
        "#             ]\n",
        "\n",
        "#     def __getitem__(self, n):\n",
        "#         waveform, sample_rate, label, *_ = super().__getitem__(n)\n",
        "#         mel = self.mel_spec(waveform).squeeze(0).transpose(0, 1)  # [time, n_mels]\n",
        "#         return mel, label\n",
        "\n",
        "# def collate_fn(batch, label_to_idx):\n",
        "#     tensors, targets = [], []\n",
        "#     for mel, label in batch:\n",
        "#         tensors.append(mel)  # [time, n_mels]\n",
        "#         targets.append(label_to_idx[label])\n",
        "\n",
        "#     tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)  # [B, max_time, n_mels]\n",
        "#     tensors = tensors.permute(0, 2, 1).unsqueeze(1)  # [B, 1, n_mels, max_time]\n",
        "#     return tensors, torch.tensor(targets)\n",
        "\n",
        "# def get_speechcommands_loaders(data_dir=\"./data\", batch_size=8, allowed_labels=None, n_mels=64):\n",
        "#     if allowed_labels is None:\n",
        "#         allowed_labels = [\"yes\", \"no\", \"up\", \"down\"]\n",
        "\n",
        "#     label_to_idx = {label: idx for idx, label in enumerate(allowed_labels)}\n",
        "\n",
        "#     train_set = FilteredSpeechCommands(\n",
        "#         root=data_dir, subset=\"training\",\n",
        "#         allowed_labels=allowed_labels, n_mels=n_mels\n",
        "#     )\n",
        "#     test_set = FilteredSpeechCommands(\n",
        "#         root=data_dir, subset=\"testing\",\n",
        "#         allowed_labels=allowed_labels, n_mels=n_mels\n",
        "#     )\n",
        "\n",
        "#     train_loader = DataLoader(\n",
        "#         train_set, batch_size=batch_size, shuffle=True,\n",
        "#         collate_fn=lambda b: collate_fn(b, label_to_idx)\n",
        "#     )\n",
        "#     test_loader = DataLoader(\n",
        "#         test_set, batch_size=batch_size, shuffle=False,\n",
        "#         collate_fn=lambda b: collate_fn(b, label_to_idx)\n",
        "#     )\n",
        "\n",
        "#     return train_loader, test_loader, allowed_labels\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchaudio\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torchaudio.transforms import MelSpectrogram, FrequencyMasking, TimeMasking\n",
        "import os\n",
        "\n",
        "class FilteredSpeechCommands(SPEECHCOMMANDS):\n",
        "    def __init__(self, root, subset=None, allowed_labels=None, n_mels=64, augment=False):\n",
        "        self.allowed_labels = allowed_labels\n",
        "        self.mel_spec = MelSpectrogram(sample_rate=16000, n_mels=n_mels)\n",
        "\n",
        "        # Add augmentation transforms if enabled\n",
        "        self.augment = augment\n",
        "        if self.augment:\n",
        "            self.freq_mask = FrequencyMasking(freq_mask_param=15)\n",
        "            self.time_mask = TimeMasking(time_mask_param=35)\n",
        "\n",
        "        dataset_path = os.path.join(root, \"speechcommands\")\n",
        "        download_flag = not os.path.exists(dataset_path)\n",
        "        super().__init__(root=dataset_path, download=download_flag, subset=subset)\n",
        "\n",
        "        if self.allowed_labels:\n",
        "            self._walker = [\n",
        "                w for w in self._walker\n",
        "                if os.path.basename(os.path.dirname(w)) in self.allowed_labels\n",
        "            ]\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        waveform, sample_rate, label, *_ = super().__getitem__(n)\n",
        "        mel = self.mel_spec(waveform).squeeze(0).transpose(0, 1)  # [time, n_mels]\n",
        "\n",
        "        # Apply augmentation only on training set\n",
        "        if self.augment:\n",
        "            mel = mel.transpose(0, 1)  # [n_mels, time]\n",
        "            mel = self.freq_mask(mel)\n",
        "            mel = self.time_mask(mel)\n",
        "            mel = mel.transpose(0, 1)  # [time, n_mels]\n",
        "\n",
        "        return mel, label\n",
        "\n",
        "def collate_fn(batch, label_to_idx):\n",
        "    tensors, targets = [], []\n",
        "    for mel, label in batch:\n",
        "        tensors.append(mel)  # [time, n_mels]\n",
        "        targets.append(label_to_idx[label])\n",
        "\n",
        "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)  # [B, max_time, n_mels]\n",
        "    tensors = tensors.permute(0, 2, 1).unsqueeze(1)  # [B, 1, n_mels, max_time]\n",
        "    return tensors, torch.tensor(targets)\n",
        "\n",
        "def get_speechcommands_loaders(data_dir=\"./data\", batch_size=8, allowed_labels=None, n_mels=64):\n",
        "    if allowed_labels is None:\n",
        "        allowed_labels = [\"yes\", \"no\", \"up\", \"down\"]\n",
        "\n",
        "    label_to_idx = {label: idx for idx, label in enumerate(allowed_labels)}\n",
        "\n",
        "    # Training set with augmentation\n",
        "    train_set = FilteredSpeechCommands(\n",
        "        root=data_dir, subset=\"training\",\n",
        "        allowed_labels=allowed_labels, n_mels=n_mels, augment=True\n",
        "    )\n",
        "    # Test set without augmentation\n",
        "    test_set = FilteredSpeechCommands(\n",
        "        root=data_dir, subset=\"testing\",\n",
        "        allowed_labels=allowed_labels, n_mels=n_mels, augment=False\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_set, batch_size=batch_size, shuffle=True,\n",
        "        collate_fn=lambda b: collate_fn(b, label_to_idx)\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_set, batch_size=batch_size, shuffle=False,\n",
        "        collate_fn=lambda b: collate_fn(b, label_to_idx)\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, allowed_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f7f8c4",
      "metadata": {
        "id": "68f7f8c4"
      },
      "source": [
        "## 4) Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca513f8d",
      "metadata": {
        "id": "ca513f8d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Adaptive pooling to avoid size issues\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [B, 1, Mel, Time]\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b659e08",
      "metadata": {
        "id": "2b659e08"
      },
      "source": [
        "## 5) Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74af645a",
      "metadata": {
        "id": "74af645a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import os\n",
        "from utils import get_speechcommands_loaders\n",
        "from models import SimpleCNN\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += y.size(0)\n",
        "            correct += predicted.eq(y).sum().item()\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    labels = [\"yes\", \"no\", \"up\", \"down\"]\n",
        "    trainloader, testloader, _ = get_speechcommands_loaders(\n",
        "        args.data_dir, args.batch_size, labels, n_mels=64\n",
        "    )\n",
        "\n",
        "    model = SimpleCNN(num_classes=len(labels)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(args.epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for X, y in trainloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += y.size(0)\n",
        "            correct += predicted.eq(y).sum().item()\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, testloader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{args.epochs} \"\n",
        "              f\"| Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} \"\n",
        "              f\"| Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}\")\n",
        "\n",
        "        # Save checkpoint for current epoch\n",
        "        ckpt_path = os.path.join(args.out_dir, f\"asr_epoch_{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"labels\": labels,\n",
        "            \"model_type\": \"cnn\"\n",
        "        }, ckpt_path)\n",
        "\n",
        "        # Save best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_path = os.path.join(args.out_dir, \"asr_best.pt\")\n",
        "            torch.save({\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"labels\": labels,\n",
        "                \"model_type\": \"cnn\"\n",
        "            }, best_path)\n",
        "            print(f\"[INFO] Best model updated with acc={best_acc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b737733",
      "metadata": {
        "id": "8b737733"
      },
      "source": [
        "## 6) Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "245ce2d3",
      "metadata": {
        "id": "245ce2d3"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from models import SimpleCNN  # your actual model\n",
        "\n",
        "def main(args):\n",
        "    # Load checkpoint\n",
        "    ckpt = torch.load(args.ckpt, map_location=\"cpu\", weights_only=True)\n",
        "    labels = ckpt[\"labels\"]\n",
        "    num_classes = len(labels)\n",
        "\n",
        "    # Create model and load weights\n",
        "    model = SimpleCNN(num_classes)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess audio\n",
        "    waveform, sr = torchaudio.load(args.audio)\n",
        "    waveform = T.Resample(sr, 16000)(waveform)  # resample to 16k\n",
        "    mel_spec = T.MelSpectrogram(sample_rate=16000, n_mels=64)(waveform)\n",
        "    mel_spec = mel_spec.unsqueeze(0)  # add batch dim\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(mel_spec)\n",
        "        pred_idx = outputs.argmax(1).item()\n",
        "        probs = torch.softmax(outputs, 1)[0].tolist()\n",
        "        print(f\"Predicted: {labels[pred_idx]} (Probs: {probs})\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd746270",
      "metadata": {
        "id": "fd746270"
      },
      "source": [
        "# Sentiment Analysis with RNN and Transformer\n",
        "This notebook trains two models (an RNN-based LSTM/GRU and a Transformer) from scratch on the IMDB movie review dataset to classify sentiment as positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66eea134",
      "metadata": {
        "id": "66eea134"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoervkUY0whx",
        "outputId": "82a17bc5-fb60-4860-a8fb-14cc7c6ef50e"
      },
      "id": "GoervkUY0whx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524ad2c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "524ad2c0",
        "outputId": "2ba84431-d354-431b-9ce2-5e9ad56ed139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext matplotlib --quiet\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d816a89",
      "metadata": {
        "id": "8d816a89"
      },
      "source": [
        "## 2. Data Download and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e3e1db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71e3e1db",
        "outputId": "2daa0462-0926-4605-9217-d9614f84f254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  44.9M      0  0:00:01  0:00:01 --:--:-- 44.9M\n"
          ]
        }
      ],
      "source": [
        "# Download and extract the IMDB dataset\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c8ef993",
      "metadata": {
        "id": "2c8ef993"
      },
      "source": [
        "## 3. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d31d42ab",
      "metadata": {
        "id": "d31d42ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------\n",
        "# Device helper\n",
        "# -------------------------\n",
        "def get_device():\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "# -------------------------\n",
        "# Simple tokenizer & vocab\n",
        "# -------------------------\n",
        "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    return TOKEN_RE.findall(text)\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, max_size: int = 20000, min_freq: int = 2, specials: List[str] = None):\n",
        "        self.max_size = max_size\n",
        "        self.min_freq = min_freq\n",
        "        self.freqs = Counter()\n",
        "        self.itos = []\n",
        "        self.stoi = {}\n",
        "        self.specials = specials or [\"<pad>\", \"<unk>\"]\n",
        "        for tok in self.specials:\n",
        "            self.add_token(tok, count=0)  # reserve positions\n",
        "\n",
        "    def add_token(self, token, count=1):\n",
        "        # used internally to reserve special tokens\n",
        "        if token not in self.freqs:\n",
        "            self.freqs[token] += count\n",
        "\n",
        "    def build_from_texts(self, texts: List[str]):\n",
        "        for t in texts:\n",
        "            toks = simple_tokenize(t)\n",
        "            self.freqs.update(toks)\n",
        "\n",
        "        # filter by min_freq\n",
        "        items = [(tok, c) for tok, c in self.freqs.items() if c >= self.min_freq and tok not in self.specials]\n",
        "        items.sort(key=lambda x: (-x[1], x[0]))\n",
        "        items = items[: self.max_size - len(self.specials)]\n",
        "        self.itos = list(self.specials) + [t for t, _ in items]\n",
        "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def encode(self, tokens: List[str], max_len: int = None) -> List[int]:\n",
        "        ids = [self.stoi.get(t, self.stoi.get(\"<unk>\")) for t in tokens]\n",
        "        if max_len is not None:\n",
        "            if len(ids) >= max_len:\n",
        "                ids = ids[:max_len]\n",
        "            else:\n",
        "                ids = ids + [self.stoi[\"<pad>\"]] * (max_len - len(ids))\n",
        "        return ids\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump({\"itos\": self.itos, \"max_size\": self.max_size, \"min_freq\": self.min_freq}, f, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        with open(path) as f:\n",
        "            data = json.load(f)\n",
        "        v = cls(max_size=data.get(\"max_size\", 20000), min_freq=data.get(\"min_freq\", 2))\n",
        "        v.itos = data[\"itos\"]\n",
        "        v.stoi = {tok: i for i, tok in enumerate(v.itos)}\n",
        "        return v\n",
        "\n",
        "# -------------------------\n",
        "# Dataset wrapper\n",
        "# -------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], labels: List[int], vocab: Vocab, max_len: int = 256):\n",
        "        assert len(texts) == len(labels)\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t = self.texts[idx]\n",
        "        toks = simple_tokenize(t)\n",
        "        ids = self.vocab.encode(toks, max_len=self.max_len)\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    xs = torch.stack(xs)\n",
        "    ys = torch.stack(ys)\n",
        "    return xs, ys\n",
        "\n",
        "# -------------------------\n",
        "# Basic training utilities\n",
        "# -------------------------\n",
        "def save_checkpoint(state: dict, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(state, path)\n",
        "\n",
        "def load_checkpoint(path: str, device):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    return checkpoint\n",
        "\n",
        "def calc_accuracy(preds: torch.Tensor, targets: torch.Tensor):\n",
        "    # preds: logits (batch, classes)\n",
        "    pred_labels = preds.argmax(dim=-1)\n",
        "    correct = (pred_labels == targets).sum().item()\n",
        "    return correct / targets.size(0)\n",
        "\n",
        "def plot_metrics(history: dict, out_path: str):\n",
        "    # history: {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[]}\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
        "    plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec0ca3b",
      "metadata": {
        "id": "1ec0ca3b"
      },
      "source": [
        "## 4. Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc92533",
      "metadata": {
        "id": "ddc92533"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# -------------------------\n",
        "# RNN (LSTM/GRU) classifier\n",
        "# -------------------------\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_layers=1, rnn_type=\"lstm\", bidirectional=True, num_classes=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "        if self.rnn_type == \"lstm\":\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers>1 else 0.0)\n",
        "        else:\n",
        "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers>1 else 0.0)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        factor = 2 if bidirectional else 1\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * factor, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        mask = (x != 0).float()  # pad idx = 0\n",
        "        emb = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        outputs, _ = self.rnn(emb)  # (batch, seq_len, hidden*dirs)\n",
        "        # average pooling across tokens with mask\n",
        "        outputs = outputs * mask.unsqueeze(-1)\n",
        "        summed = outputs.sum(dim=1)  # sum over seq\n",
        "        denom = mask.sum(dim=1).unsqueeze(-1).clamp(min=1.0)\n",
        "        avg = summed / denom\n",
        "        logits = self.fc(avg)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Simple Transformer classifier\n",
        "# -------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, nhead=4, num_encoder_layers=2,\n",
        "                 dim_feedforward=256, max_len=256, num_classes=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.pos_enc = PositionalEncoding(embed_dim, max_len=max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        mask = (x == 0)  # pad tokens\n",
        "\n",
        "        emb = self.embedding(x)\n",
        "        emb = self.pos_enc(emb)\n",
        "\n",
        "        # 🚫 No src_key_padding_mask here to avoid MPS nested tensor bug\n",
        "        out = self.transformer(emb)  # (batch, seq_len, embed_dim)\n",
        "\n",
        "        # Zero out pad positions manually\n",
        "        mask_float = (~mask).unsqueeze(-1).float()\n",
        "        out = out * mask_float\n",
        "\n",
        "        # Mean pool over non-pad tokens\n",
        "        summed = out.sum(dim=1)\n",
        "        denom = mask_float.sum(dim=1).clamp(min=1.0)\n",
        "        avg = summed / denom\n",
        "\n",
        "        logits = self.classifier(avg)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4816fce",
      "metadata": {
        "id": "a4816fce"
      },
      "source": [
        "## 5. Training the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019cf22a",
      "metadata": {
        "id": "019cf22a"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Train an RNN-based sentiment classifier (LSTM or GRU) from scratch on the IMDB dataset.\n",
        "\n",
        "Usage:\n",
        "    python train_sentiment.py --model lstm --epochs 6 --batch_size 64\n",
        "\"\"\"\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#from utils import get_device, Vocab, TextDataset, collate_batch, save_checkpoint, plot_metrics, simple_tokenize\n",
        "#from models import RNNClassifier\n",
        "import glob\n",
        "\n",
        "def load_imdb_fallback(data_dir=\"aclImdb\", split=\"train\"):\n",
        "    \"\"\"\n",
        "    Loads IMDB dataset from a local folder in the Stanford 'aclImdb' format.\n",
        "    data_dir: path to 'aclImdb' folder\n",
        "    split: 'train' or 'test'\n",
        "    Returns: (texts, labels) where labels are 0=neg, 1=pos\n",
        "    \"\"\"\n",
        "    texts, labels = [], []\n",
        "    for label_name, label_val in [(\"neg\", 0), (\"pos\", 1)]:\n",
        "        path_pattern = os.path.join(data_dir, split, label_name, \"*.txt\")\n",
        "        for file_path in glob.glob(path_pattern):\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                texts.append(f.read())\n",
        "                labels.append(label_val)\n",
        "    return texts, labels\n",
        "\n",
        "def load_imdb_locally(split=\"train\"):\n",
        "    \"\"\"\n",
        "    Try torchtext.datasets.IMDB first.\n",
        "    If unavailable, load from local folder.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from torchtext.datasets import IMDB\n",
        "        ds = list(IMDB(root=\".data\", split=split))\n",
        "        texts = [t for label, t in ds]\n",
        "        labels = [0 if label.lower().startswith(\"neg\") else 1 for label, t in ds]\n",
        "        return texts, labels\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] torchtext IMDB not available, falling back to local dataset.\")\n",
        "        if not os.path.exists(\"aclImdb\"):\n",
        "            raise RuntimeError(\n",
        "                \"Local IMDB dataset not found. Please download from \"\n",
        "                \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \"\n",
        "                \"and extract into ./aclImdb\"\n",
        "            )\n",
        "        return load_imdb_fallback(data_dir=\"aclImdb\", split=split)\n",
        "\n",
        "\n",
        "def split_train_val(texts, labels, val_frac=0.1, seed=42):\n",
        "    idx = list(range(len(texts)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    cut = int(len(idx)*(1-val_frac))\n",
        "    train_idx = idx[:cut]\n",
        "    val_idx = idx[cut:]\n",
        "    train_texts = [texts[i] for i in train_idx]\n",
        "    train_labels = [labels[i] for i in train_idx]\n",
        "    val_texts = [texts[i] for i in val_idx]\n",
        "    val_labels = [labels[i] for i in val_idx]\n",
        "    return train_texts, train_labels, val_texts, val_labels\n",
        "\n",
        "def train_epoch(model, loader, opt, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    n = 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        batch_size = x.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_acc += (logits.argmax(dim=-1) == y).sum().item()\n",
        "        n += batch_size\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "def eval_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            batch_size = x.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "            total_acc += (logits.argmax(dim=-1) == y).sum().item()\n",
        "            n += batch_size\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "def main(args):\n",
        "    device = get_device()\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Load IMDB\n",
        "    texts, labels = load_imdb_locally(split=\"train\")\n",
        "    print(\"Loaded IMDB train size:\", len(texts))\n",
        "\n",
        "    # small subset option for quick experiments\n",
        "    if args.limit and args.limit > 0:\n",
        "        texts = texts[:args.limit]\n",
        "        labels = labels[:args.limit]\n",
        "\n",
        "    train_texts, train_labels, val_texts, val_labels = split_train_val(texts, labels, val_frac=args.val_frac)\n",
        "\n",
        "    # build vocab\n",
        "    print(\"Building vocab...\")\n",
        "    vocab = Vocab(max_size=args.vocab_size, min_freq=args.min_freq)\n",
        "    vocab.build_from_texts(train_texts)\n",
        "    print(\"Vocab size:\", len(vocab))\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    vocab.save(os.path.join(args.out_dir, \"vocab.json\"))\n",
        "\n",
        "    # datasets and loaders\n",
        "    train_ds = TextDataset(train_texts, train_labels, vocab, max_len=args.max_len)\n",
        "    val_ds = TextDataset(val_texts, val_labels, vocab, max_len=args.max_len)\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "    # model\n",
        "    model = RNNClassifier(vocab_size=len(vocab), embed_dim=args.embed_dim, hidden_dim=args.hidden_dim, num_layers=args.num_layers, rnn_type=args.rnn_type, bidirectional=args.bidirectional, num_classes=2, dropout=args.dropout)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, criterion, device)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        print(f\"Epoch {epoch}/{args.epochs} -- train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "        # checkpoint\n",
        "        ckpt_path = os.path.join(args.out_dir, f\"model_epoch{epoch}.pt\")\n",
        "        save_checkpoint({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"vocab\": vocab.itos,\n",
        "            \"history\": history\n",
        "        }, ckpt_path)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            save_checkpoint({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"vocab\": vocab.itos,\n",
        "                \"history\": history\n",
        "            }, os.path.join(args.out_dir, \"best_model.pt\"))\n",
        "    # save final history plot\n",
        "    plot_metrics(history, os.path.join(args.out_dir, \"training_metrics.png\"))\n",
        "    print(\"Training complete. Best val acc:\", best_val_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3395c978",
      "metadata": {
        "id": "3395c978"
      },
      "source": [
        "## 6. Training the Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9317c35b",
      "metadata": {
        "id": "9317c35b"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Train a simple Transformer encoder classifier on IMDB dataset.\n",
        "Safe for Apple Silicon MPS (no nested tensor mask bug).\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#from utils import get_device, Vocab, TextDataset, collate_batch, save_checkpoint, plot_metrics\n",
        "#from models import TransformerClassifier\n",
        "import glob\n",
        "\n",
        "# -------------------------\n",
        "# Local IMDB loader\n",
        "# -------------------------\n",
        "def load_imdb_fallback(data_dir=\"aclImdb\", split=\"train\"):\n",
        "    texts, labels = [], []\n",
        "    for label_name, label_val in [(\"neg\", 0), (\"pos\", 1)]:\n",
        "        path_pattern = os.path.join(data_dir, split, label_name, \"*.txt\")\n",
        "        for file_path in glob.glob(path_pattern):\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                texts.append(f.read())\n",
        "                labels.append(label_val)\n",
        "    return texts, labels\n",
        "\n",
        "def load_imdb_locally(split=\"train\"):\n",
        "    try:\n",
        "        from torchtext.datasets import IMDB\n",
        "        ds = list(IMDB(root=\".data\", split=split))\n",
        "        texts = [t for label, t in ds]\n",
        "        labels = [0 if label.lower().startswith(\"neg\") else 1 for label, t in ds]\n",
        "        return texts, labels\n",
        "    except Exception:\n",
        "        print(\"[WARN] torchtext IMDB not available, falling back to local dataset.\")\n",
        "        if not os.path.exists(\"aclImdb\"):\n",
        "            raise RuntimeError(\n",
        "                \"Local IMDB dataset not found. Please download from:\\n\"\n",
        "                \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\\n\"\n",
        "                \"and extract into ./aclImdb\"\n",
        "            )\n",
        "        return load_imdb_fallback(data_dir=\"aclImdb\", split=split)\n",
        "\n",
        "def split_train_val(texts, labels, val_frac=0.1, seed=42):\n",
        "    idx = list(range(len(texts)))\n",
        "    random.Random(seed).shuffle(idx)\n",
        "    cut = int(len(idx) * (1 - val_frac))\n",
        "    train_idx = idx[:cut]\n",
        "    val_idx = idx[cut:]\n",
        "    return (\n",
        "        [texts[i] for i in train_idx],\n",
        "        [labels[i] for i in train_idx],\n",
        "        [texts[i] for i in val_idx],\n",
        "        [labels[i] for i in val_idx],\n",
        "    )\n",
        "\n",
        "# -------------------------\n",
        "# Training / Eval loops\n",
        "# -------------------------\n",
        "def train_epoch(model, loader, opt, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(x)  # No src_key_padding_mask used in model\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        total_acc += (logits.argmax(dim=-1) == y).sum().item()\n",
        "        n += x.size(0)\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "def eval_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            total_acc += (logits.argmax(dim=-1) == y).sum().item()\n",
        "            n += x.size(0)\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "def main(args):\n",
        "    device = get_device()\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    texts, labels = load_imdb_locally(split=\"train\")\n",
        "    print(\"Loaded IMDB train size:\", len(texts))\n",
        "\n",
        "    if args.limit and args.limit > 0:\n",
        "        texts, labels = texts[:args.limit], labels[:args.limit]\n",
        "\n",
        "    train_texts, train_labels, val_texts, val_labels = split_train_val(\n",
        "        texts, labels, val_frac=args.val_frac\n",
        "    )\n",
        "\n",
        "    print(\"Building vocab...\")\n",
        "    vocab = Vocab(max_size=args.vocab_size, min_freq=args.min_freq)\n",
        "    vocab.build_from_texts(train_texts)\n",
        "    print(\"Vocab size:\", len(vocab))\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    vocab.save(os.path.join(args.out_dir, \"vocab.json\"))\n",
        "\n",
        "    train_ds = TextDataset(train_texts, train_labels, vocab, max_len=args.max_len)\n",
        "    val_ds = TextDataset(val_texts, val_labels, vocab, max_len=args.max_len)\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_batch\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    model = TransformerClassifier(\n",
        "        vocab_size=len(vocab),\n",
        "        embed_dim=args.embed_dim,\n",
        "        nhead=args.nhead,\n",
        "        num_encoder_layers=args.num_layers,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        max_len=args.max_len,\n",
        "        num_classes=2,\n",
        "        dropout=args.dropout,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, criterion, device)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{args.epochs} -- \"\n",
        "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
        "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        ckpt = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"vocab\": vocab.itos,\n",
        "            \"history\": history,\n",
        "        }\n",
        "        torch.save(ckpt, os.path.join(args.out_dir, f\"transformer_epoch{epoch}.pt\"))\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(ckpt, os.path.join(args.out_dir, \"transformer_best.pt\"))\n",
        "\n",
        "    plot_metrics(history, os.path.join(args.out_dir, \"transformer_training_metrics.png\"))\n",
        "    print(\"Training complete. Best val acc:\", best_val_acc)\n",
        "\n",
        "# -------------------------\n",
        "# Entry point\n",
        "# -------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3245c9fd",
      "metadata": {
        "id": "3245c9fd"
      },
      "source": [
        "## 7. Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0cca2d",
      "metadata": {
        "id": "0b0cca2d"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from utils import get_device, Vocab, simple_tokenize\n",
        "from models import RNNClassifier, TransformerClassifier\n",
        "\n",
        "def detect_model_type(state_dict):\n",
        "    if any(k.startswith(\"rnn.\") for k in state_dict.keys()):\n",
        "        return \"rnn\"\n",
        "    elif any(k.startswith(\"transformer.\") for k in state_dict.keys()):\n",
        "        return \"transformer\"\n",
        "    else:\n",
        "        raise ValueError(\"Cannot detect model type from checkpoint keys\")\n",
        "\n",
        "def main(args):\n",
        "    device = get_device()\n",
        "    ckpt = torch.load(args.ckpt, map_location=device)\n",
        "\n",
        "    # rebuild vocab\n",
        "    vocab = Vocab()\n",
        "    vocab.itos = ckpt[\"vocab\"]\n",
        "    vocab.stoi = {tok: i for i, tok in enumerate(vocab.itos)}\n",
        "\n",
        "    model_type = detect_model_type(ckpt[\"model_state\"])\n",
        "    print(f\"[INFO] Detected model type: {model_type}\")\n",
        "\n",
        "    # crude guessing of config from weights\n",
        "    if model_type == \"rnn\":\n",
        "        fc_in_features = ckpt[\"model_state\"][\"fc.0.weight\"].shape[1]\n",
        "        bidirectional = \"rnn.weight_ih_l0_reverse\" in ckpt[\"model_state\"]\n",
        "        hidden_dim = fc_in_features // (2 if bidirectional else 1)\n",
        "        model = RNNClassifier(\n",
        "            vocab_size=len(vocab),\n",
        "            embed_dim=ckpt[\"model_state\"][\"embedding.weight\"].shape[1],\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=1,  # default guess\n",
        "            rnn_type=\"lstm\",\n",
        "            bidirectional=bidirectional,\n",
        "            num_classes=2,\n",
        "            dropout=0.2\n",
        "        )\n",
        "    else:  # transformer\n",
        "        embed_dim = ckpt[\"model_state\"][\"embedding.weight\"].shape[1]\n",
        "        # assume nhead=4, dim_feedforward=256, num_layers=2 (matches your training default)\n",
        "        model = TransformerClassifier(\n",
        "            vocab_size=len(vocab),\n",
        "            embed_dim=embed_dim,\n",
        "            nhead=4,\n",
        "            num_encoder_layers=2,\n",
        "            dim_feedforward=256,\n",
        "            max_len=256,\n",
        "            num_classes=2,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # tokenize & encode\n",
        "    toks = simple_tokenize(args.text)\n",
        "    ids = vocab.encode(toks, max_len=256)\n",
        "    inp = torch.tensor([ids], dtype=torch.long).to(device)\n",
        "\n",
        "    # inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(inp)\n",
        "        probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "        pred = int(probs.argmax())\n",
        "\n",
        "    label_map = {0: \"negative\", 1: \"positive\"}\n",
        "    print(f\"Input: {args.text}\")\n",
        "    print(f\"Predicted: {label_map[pred]} (probs: {probs.tolist()})\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
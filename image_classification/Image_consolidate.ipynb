{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa6eedab",
      "metadata": {
        "id": "fa6eedab"
      },
      "source": [
        "# Image Classification on CIFAR-10 with ResNet & ViT\n",
        "This notebook consolidates the full pipeline for training and evaluating **ResNet** and **Vision Transformer (ViT)** models on the **CIFAR-10** dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee54939",
      "metadata": {
        "id": "1ee54939"
      },
      "source": [
        "## 1) Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9473620",
      "metadata": {
        "id": "e9473620"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torch torchvision matplotlib --quiet\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b269381",
      "metadata": {
        "id": "3b269381"
      },
      "source": [
        "## 2) Data: CIFAR-10 Download & Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e4bfdda",
      "metadata": {
        "id": "3e4bfdda"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import torchvision\n",
        "\n",
        "DATA_DIR = \"./data/cifar10\"\n",
        "\n",
        "def main():\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    print(f\"[INFO] Downloading CIFAR-10 dataset to {DATA_DIR} ...\")\n",
        "    torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True)\n",
        "    torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True)\n",
        "    print(\"[INFO] CIFAR-10 dataset downloaded successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e982fd7",
      "metadata": {
        "id": "5e982fd7"
      },
      "source": [
        "## 3) Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d11387",
      "metadata": {
        "id": "01d11387"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def get_cifar10_loaders(batch_size=128, data_dir=\"./data/cifar10\", num_workers=2):\n",
        "    \"\"\"\n",
        "    Loads CIFAR-10 from local data_dir (dataset must be pre-downloaded).\n",
        "    \"\"\"\n",
        "    if not os.path.exists(os.path.join(data_dir, \"cifar-10-batches-py\")):\n",
        "        raise FileNotFoundError(\n",
        "            f\"CIFAR-10 dataset not found in {data_dir}. \"\n",
        "            f\"Run `python download_cifar10.py` first.\"\n",
        "        )\n",
        "\n",
        "    # Transforms\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Train set\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir,\n",
        "        train=True,\n",
        "        download=False,\n",
        "        transform=transform_train\n",
        "    )\n",
        "\n",
        "    # Test set\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir,\n",
        "        train=False,\n",
        "        download=False,\n",
        "        transform=transform_test\n",
        "    )\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=False\n",
        "    )\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=False\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1482d39",
      "metadata": {
        "id": "a1482d39"
      },
      "source": [
        "## 4) Model Definitions (ResNet & ViT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d594ea91",
      "metadata": {
        "id": "d594ea91"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------\n",
        "# Vision Transformer (EXACTLY your previous one)\n",
        "# -----------------\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.proj(x)  # [B, embed_dim, H', W']\n",
        "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10,\n",
        "                 embed_dim=128, depth=6, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.norm(x[:, 0])  # CLS token\n",
        "        return self.head(x)\n",
        "\n",
        "# -----------------\n",
        "# Small ResNet\n",
        "# -----------------\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class SmallResNet(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, num_blocks=[2, 2, 2], num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride):\n",
        "        layers = [block(self.in_channels, out_channels, stride)]\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be75c385",
      "metadata": {
        "id": "be75c385"
      },
      "source": [
        "## 5) Training Pipeline: ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e082ca",
      "metadata": {
        "id": "f0e082ca"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from utils import get_cifar10_loaders\n",
        "from models import SmallResNet\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
        "                          else \"cuda\" if torch.cuda.is_available()\n",
        "                          else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    trainloader, testloader = get_cifar10_loaders(batch_size=args.batch_size)\n",
        "\n",
        "    model = SmallResNet(num_classes=args.num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        # train\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0.0, 0, 0\n",
        "        for imgs, labels in trainloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # val\n",
        "        model.eval()\n",
        "        val_correct, val_total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in testloader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{args.epochs}, \"\n",
        "              f\"Train Loss: {total_loss/total:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            ckpt_path = os.path.join(args.out_dir, \"resnet_best.pt\")\n",
        "            torch.save({\n",
        "                \"model_type\": \"resnet\",\n",
        "                \"model_config\": {\"num_classes\": args.num_classes},\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"val_acc\": val_acc\n",
        "            }, ckpt_path)\n",
        "            print(f\"[INFO] Saved new best model to {ckpt_path} (Val Acc: {val_acc:.4f})\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027b3645",
      "metadata": {
        "id": "027b3645"
      },
      "source": [
        "## 6) Training Pipeline: Vision Transformer (ViT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24938920",
      "metadata": {
        "id": "24938920"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from utils import get_cifar10_loaders\n",
        "from models import VisionTransformer\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
        "                          else \"cuda\" if torch.cuda.is_available()\n",
        "                          else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    trainloader, testloader = get_cifar10_loaders(batch_size=args.batch_size)\n",
        "\n",
        "    model = VisionTransformer(num_classes=args.num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        # train\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0.0, 0, 0\n",
        "        for imgs, labels in trainloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # val\n",
        "        model.eval()\n",
        "        val_correct, val_total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in testloader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{args.epochs}, \"\n",
        "              f\"Train Loss: {total_loss/total:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            ckpt_path = os.path.join(args.out_dir, \"vit_best.pt\")\n",
        "            torch.save({\n",
        "                \"model_type\": \"vit\",\n",
        "                \"model_config\": {\"num_classes\": args.num_classes},\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"val_acc\": val_acc\n",
        "            }, ckpt_path)\n",
        "            print(f\"[INFO] Saved new best model to {ckpt_path} (Val Acc: {val_acc:.4f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82af8333",
      "metadata": {
        "id": "82af8333"
      },
      "source": [
        "## 7) Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162669b1",
      "metadata": {
        "id": "162669b1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import argparse\n",
        "from models import VisionTransformer, SmallResNet\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "CLASSES = ['plane','car','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "def predict_image(path, ckpt_path):\n",
        "    # Load checkpoint\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model_type = ckpt.get(\"model_type\", \"resnet\")  # Default to resnet if missing\n",
        "    num_classes = ckpt.get(\"model_config\", {}).get(\"num_classes\", 10)\n",
        "\n",
        "    # Load model\n",
        "    if model_type == \"vit\":\n",
        "        model = VisionTransformer(num_classes=num_classes).to(device)\n",
        "    else:\n",
        "        model = SmallResNet(num_classes=num_classes).to(device)\n",
        "\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        pred_idx = probs.argmax(1).item()\n",
        "\n",
        "    print(f\"Predicted: {CLASSES[pred_idx]} (Prob: {probs[0][pred_idx].item():.4f})\")\n",
        "    return CLASSES[pred_idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a836311",
      "metadata": {
        "id": "2a836311"
      },
      "source": [
        "## 8) Quick Start (Summary Logs Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d97239b",
      "metadata": {
        "id": "6d97239b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example: quick sanity run with small epoch count\n",
        "# You can modify these calls to run full experiments.\n",
        "# NOTE: Keep epochs low in Colab for demo; increase for real training.\n",
        "try:\n",
        "    # ResNet quick run (adjust args as required by your training functions)\n",
        "    print(\"=== Quick ResNet Training (demo) ===\")\n",
        "    # e.g., train_resnet(epochs=2, batch_size=128, lr=0.001, out_dir=\"checkpoints_resnet_demo\")\n",
        "except Exception as e:\n",
        "    print(\"ResNet demo skipped:\", e)\n",
        "\n",
        "try:\n",
        "    # ViT quick run\n",
        "    print(\"=== Quick ViT Training (demo) ===\")\n",
        "    # e.g., train_vit(epochs=2, batch_size=128, lr=0.001, out_dir=\"checkpoints_vit_demo\")\n",
        "except Exception as e:\n",
        "    print(\"ViT demo skipped:\", e)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}